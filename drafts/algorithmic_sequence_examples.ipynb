{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import circuitsvis as cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36315/1892135215.py:4: DeprecationWarning: `magic(...)` is deprecated since IPython 0.13 (warning added in 8.1), use run_line_magic(magic_name, parameter_s).\n",
      "  ipython.magic(\"load_ext autoreload\")\n",
      "/tmp/ipykernel_36315/1892135215.py:5: DeprecationWarning: `magic(...)` is deprecated since IPython 0.13 (warning added in 8.1), use run_line_magic(magic_name, parameter_s).\n",
      "  ipython.magic(\"autoreload 2\")\n"
     ]
    }
   ],
   "source": [
    "from IPython import get_ipython\n",
    "ipython = get_ipython()\n",
    "# Code to automatically update the HookedTransformer code as its edited without restarting the kernel\n",
    "ipython.magic(\"load_ext autoreload\")\n",
    "ipython.magic(\"autoreload 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.io as pio\n",
    "pio.renderers.default = \"browser\" # or use \"browser\" if you want plots to open with browser\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import einops\n",
    "from fancy_einsum import einsum\n",
    "from dataclasses import dataclass\n",
    "from einops import rearrange\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "import random\n",
    "from pathlib import Path\n",
    "import plotly.express as px\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from torchtyping import TensorType as TT\n",
    "from typing import List, Union, Optional\n",
    "from functools import partial\n",
    "import copy\n",
    "\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "import transformer_lens as tl\n",
    "import transformer_lens.utils as utils\n",
    "from transformer_lens.hook_points import HookedRootModule, HookPoint  # Hooking utilities\n",
    "from transformer_lens import HookedTransformer, HookedTransformerConfig, FactoredMatrix, ActivationCache\n",
    "\n",
    "def imshow(tensor, renderer=None, xaxis=\"\", yaxis=\"\", **kwargs):\n",
    "    px.imshow(utils.to_numpy(tensor), color_continuous_midpoint=0.0, color_continuous_scale=\"RdBu\", labels={\"x\":xaxis, \"y\":yaxis}, **kwargs).show(renderer)\n",
    "    return px.imshow(utils.to_numpy(tensor), color_continuous_midpoint=0.0, color_continuous_scale=\"RdBu\", labels={\"x\":xaxis, \"y\":yaxis}, **kwargs)\n",
    "\n",
    "def line(tensor, renderer=None, xaxis=\"\", yaxis=\"\", **kwargs):\n",
    "    px.line(utils.to_numpy(tensor), labels={\"x\":xaxis, \"y\":yaxis}, **kwargs).show(renderer)\n",
    "    return px.line(utils.to_numpy(tensor), labels={\"x\":xaxis, \"y\":yaxis}, **kwargs)\n",
    "\n",
    "def scatter(x, y, xaxis=\"\", yaxis=\"\", caxis=\"\", renderer=None, **kwargs):\n",
    "    x = utils.to_numpy(x)\n",
    "    y = utils.to_numpy(y)\n",
    "    px.scatter(y=y, x=x, labels={\"x\":xaxis, \"y\":yaxis, \"color\":caxis}, **kwargs).show(renderer)\n",
    "    return px.scatter(y=y, x=x, labels={\"x\":xaxis, \"y\":yaxis, \"color\":caxis}, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_fibonacci_sequences(seq_len, max_start):\n",
    "    x, y = [], []\n",
    "    for i in range(0, max_start):\n",
    "        seq = [i, i+1]\n",
    "        for j in range(2, seq_len+1):\n",
    "            seq.append(seq[j-1] + seq[j-2])\n",
    "        x.append(seq[:seq_len])\n",
    "        y.append(seq[1:])\n",
    "    return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_fibmodp_sequences(seq_len, dataset_size, p):\n",
    "    \n",
    "    combined_seq = [0, 1]\n",
    "    for i in range(2, dataset_size):\n",
    "        combined_seq.append((combined_seq[-1] + combined_seq[-2]) % p)\n",
    "    \n",
    "    x, y = [], []\n",
    "    for i in range(0, dataset_size-seq_len-1):\n",
    "        x.append(combined_seq[i:i+seq_len])\n",
    "        y.append(combined_seq[i+1:i+seq_len+1])\n",
    " \n",
    "    return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lookback_addition_sequences(seq_len, max_start, k=1):\n",
    "    x, y = [], []\n",
    "    for i in range(0, max_start):\n",
    "        seq = [i, i+1]\n",
    "        for j in range(2, seq_len+1):\n",
    "            seq.append(seq[j-k] + 1)\n",
    "        x.append(seq[:seq_len])\n",
    "        y.append(seq[1:])\n",
    "    return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mod3_sequences(seq_len, max_start):\n",
    "    x, y = [], []\n",
    "    for i in range(0, max_start):\n",
    "        seq = [i]\n",
    "        for j in range(1, seq_len+1):\n",
    "            if seq[j-1] % 3 == 0:\n",
    "                seq.append(seq[j-1]+1)\n",
    "            else:\n",
    "                seq.append(seq[j-1]+2)\n",
    "        x.append(seq[:seq_len])\n",
    "        y.append(seq[1:])\n",
    "    return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumSequenceDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        \"\"\"Initialize the dataset\n",
    "        Args:\n",
    "            x (list): list of input sequences\n",
    "            y (list): list of output sequences\n",
    "        \"\"\"\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "@dataclass\n",
    "class DataArgs():\n",
    "    max_seq_len: int\n",
    "    batch_size: int\n",
    "    num_workers: int\n",
    "    dataset_size: int\n",
    "\n",
    "@dataclass\n",
    "class TrainingArgs():\n",
    "    batch_size: int\n",
    "    epochs: int\n",
    "    optimizer: torch.optim.Optimizer\n",
    "    lr: float\n",
    "    betas: Tuple[float]\n",
    "    weight_decay: float\n",
    "    track: bool\n",
    "    cuda: bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "data_args = DataArgs(\n",
    "    max_seq_len = 128,\n",
    "    batch_size = 256,\n",
    "    num_workers = 4,\n",
    "    dataset_size = 2048\n",
    ")\n",
    "\n",
    "#x, y = build_fibonacci_sequences(args.max_seq_len, 256)\n",
    "x, y = build_fibmodp_sequences(data_args.max_seq_len, data_args.dataset_size, 100)\n",
    "#x, y = build_mod3_sequences(args.max_seq_len, 512)\n",
    "dataset = NumSequenceDataset(x, y)\n",
    "vocab_size = int(torch.max(y).item())+1\n",
    "print(vocab_size)\n",
    "\n",
    "train_set, val_set = torch.utils.data.random_split(dataset, [int(len(dataset)*0.8), int(len(dataset) - int(len(dataset)*0.8))])\n",
    "\n",
    "trainloader = DataLoader(train_set, batch_size=data_args.batch_size, shuffle=True, num_workers=4)\n",
    "valloader = DataLoader(val_set, batch_size=data_args.batch_size, shuffle=False, num_workers=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tl.EasyTransformerConfig(\n",
    "    d_model=64,\n",
    "    d_head=32,\n",
    "    n_heads=2,\n",
    "    d_mlp=256,\n",
    "    n_layers=1,\n",
    "    n_ctx=128,\n",
    "    #use_local_attn=True,\n",
    "    #attn_types=[\"local\",\"global\"],\n",
    "    #window_size=2,\n",
    "    act_fn=\"solu_ln\",\n",
    "    d_vocab=vocab_size,\n",
    "    normalization_type=\"LN\",\n",
    "    seed=23,\n",
    ")\n",
    "\n",
    "args = TrainingArgs(\n",
    "    batch_size = 128,\n",
    "    epochs = 50,\n",
    "    optimizer = torch.optim.AdamW,\n",
    "    lr = 0.001,\n",
    "    betas = (0.99, 0.999),\n",
    "    weight_decay = 1,\n",
    "    track = False,\n",
    "    cuda = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "MODEL_FILENAME = \"./fibonacci_model.pt\"\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def train_transformer(trainloader: DataLoader, args) -> list:\n",
    "    '''\n",
    "    Defines a Transformer from our custom modules, and trains it on the algotimic seq dataset.\n",
    "    '''\n",
    "    epochs = args.epochs\n",
    "    \n",
    "    #model = NumSequenceTransformer(args).to(device).train()\n",
    "    model = tl.EasyTransformer(config).to(device)\n",
    "    optimizer = args.optimizer(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "    loss_list = []\n",
    "    accuracy_list = []\n",
    "    #scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    #    optimizer, max_lr=0.01, steps_per_epoch=len(trainloader), epochs=args.epochs)\n",
    "    \n",
    "    progress_bar = tqdm_notebook(range(epochs))\n",
    "    for epoch in progress_bar:\n",
    "        \n",
    "        for (x, y) in trainloader:\n",
    "        #for batch in trainloader:\n",
    "            \n",
    "            #x = x.to(device)\n",
    "            #y = y.to(device)\n",
    "\n",
    "            #logits = model(x)\n",
    "            logits = model(x.long(), return_type=\"logits\")\n",
    "            logits = rearrange(logits, 'B S V -> (B S) V')\n",
    "            y = rearrange(y, 'B S -> (B S)')\n",
    "\n",
    "            loss = F.cross_entropy(logits, y)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            #scheduler.step()\n",
    "\n",
    "            loss_list.append(loss.item())\n",
    "\n",
    "            progress_bar.set_description(f\"Epoch = {epoch}, Loss = {loss.item():.4f}\")\n",
    "\n",
    "    print(f\"Saving model to: {MODEL_FILENAME}\")\n",
    "    torch.save(model, MODEL_FILENAME)\n",
    "    return model, loss_list, accuracy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  cpu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5feff6f0d7445619efaec163f77bc99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to: ./fibonacci_model.pt\n",
      "Opening in existing browser session.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1216/142421.923285:ERROR:file_io_posix.cc(152)] open /home/curttigges/.config/BraveSoftware/Brave-Browser/Crash Reports/pending/0e0868d6-92df-4383-b1e1-85e8fe993f22.lock: File exists (17)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libva error: /usr/lib/x86_64-linux-gnu/dri/i965_drv_video.so init failed\n",
      "[38895:38895:0100/000000.159654:ERROR:sandbox_linux.cc(376)] InitializeSandbox() called with multiple threads in process gpu-process.\n"
     ]
    }
   ],
   "source": [
    "model, loss_list, accuracy_list = train_transformer(trainloader, args=args)\n",
    "\n",
    "fig = px.line(y=loss_list, template=\"simple_white\")\n",
    "fig.update_layout(title=\"Cross entropy loss on Fibonacci\", yaxis_range=[0, max(loss_list)])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[84, 81, 65, 46, 11, 57, 68, 25, 93, 18, 11, 29, 40, 69, 9, 78, 87, 65, 52, 17, 69, 86, 55, 41, 96, 37, 33, 70, 3, 73, 76, 49, 25, 74, 99, 73, 72, 45, 17, 62, 79, 41, 20, 61, 81, 42, 23, 65, 88, 53, 41, 94, 35, 29, 64, 93, 57, 50, 7, 57, 64, 21, 85, 6, 91, 97, 88, 85, 73, 58, 31, 89, 20, 9, 29, 38, 67, 5, 72, 77, 49, 26, 75, 1, 76, 77, 53, 30, 83, 13, 96, 9, 5, 14, 19, 33, 52, 85, 37, 22, 59, 81, 40, 21, 61]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('common_modules/')\n",
    "\n",
    "import sample_methods as s\n",
    "\n",
    "model = torch.load(MODEL_FILENAME, map_location=torch.device('cpu'))\n",
    "model.eval()\n",
    "\n",
    "initial_seq = [84, 81, 65, 46, 11]\n",
    "\n",
    "output = s.sample_tokens_no_detokenization(\n",
    "    model, initial_seq, max_tokens_generated=100, max_seq_len=data_args.max_seq_len, \n",
    "    temperature=0, top_k=10\n",
    ")\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_input = dataset.__getitem__(1)[0].long()[:20]\n",
    "loss, cache = model.run_with_cache(example_input, return_type=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformer_lens.ActivationCache.ActivationCache'>\n",
      "torch.Size([2, 20, 20])\n",
      "Layer 0 Head Attention Patterns:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-41571745-c90a\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, AttentionHeads } from \"https://unpkg.com/circuitsvis@1.34.0/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-41571745-c90a\",\n",
       "      AttentionHeads,\n",
       "      {\"attention\": [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5105458498001099, 0.4894541800022125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2969231903553009, 0.46872419118881226, 0.23435261845588684, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3125990629196167, 0.26137036085128784, 0.24962693452835083, 0.1764037013053894, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2964755594730377, 0.25911614298820496, 0.17610369622707367, 0.1735335737466812, 0.09477104991674423, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.07615850865840912, 0.07821407914161682, 0.15143853425979614, 0.23533391952514648, 0.1330961436033249, 0.32575881481170654, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.18937325477600098, 0.19005899131298065, 0.17064936459064484, 0.1987210363149643, 0.1093224436044693, 0.03340505063533783, 0.10846979916095734, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.10510168224573135, 0.12080147862434387, 0.13866126537322998, 0.18503092229366302, 0.1453315168619156, 0.08181200176477432, 0.10388683527708054, 0.11937423050403595, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.08908142149448395, 0.11566869169473648, 0.1745741218328476, 0.15191440284252167, 0.08023679256439209, 0.11397600919008255, 0.09082357585430145, 0.10222427546977997, 0.08150072395801544, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0777134895324707, 0.0719614028930664, 0.055558398365974426, 0.14874829351902008, 0.07646088302135468, 0.035689856857061386, 0.10583426058292389, 0.06723297387361526, 0.13661383092403412, 0.22418652474880219, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1379709094762802, 0.10772240161895752, 0.0903329998254776, 0.08326445519924164, 0.06664196401834488, 0.046961963176727295, 0.09353898465633392, 0.0635036826133728, 0.08861490339040756, 0.13679130375385284, 0.08465646952390671, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.07088346034288406, 0.06685841828584671, 0.026064682751893997, 0.08731560409069061, 0.0772990733385086, 0.04417305812239647, 0.06367426365613937, 0.10922873020172119, 0.08746163547039032, 0.1950884312391281, 0.06067446991801262, 0.11127825826406479, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.06936390697956085, 0.049421440809965134, 0.054823391139507294, 0.07259576767683029, 0.06022017076611519, 0.02437378279864788, 0.051508307456970215, 0.03041301667690277, 0.2126244157552719, 0.1892414540052414, 0.06744665652513504, 0.057080529630184174, 0.060887131839990616, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.049103181809186935, 0.05114724487066269, 0.09953010827302933, 0.12899315357208252, 0.04603772237896919, 0.03888241946697235, 0.08114320784807205, 0.04703477770090103, 0.06033867225050926, 0.05078728124499321, 0.05682607740163803, 0.1267598420381546, 0.08385135978460312, 0.07956504076719284, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.08240793645381927, 0.09291862696409225, 0.10784447193145752, 0.045893777161836624, 0.036026906222105026, 0.03279585763812065, 0.08039501309394836, 0.07402978837490082, 0.03339653089642525, 0.057645779103040695, 0.04028264060616493, 0.1533423811197281, 0.04825664684176445, 0.09357640892267227, 0.02118724212050438, 0.0, 0.0, 0.0, 0.0, 0.0], [0.05140009894967079, 0.04929836839437485, 0.06760788708925247, 0.09423266351222992, 0.039688512682914734, 0.06214618682861328, 0.03402896970510483, 0.04103263467550278, 0.05964349955320358, 0.030775414779782295, 0.14027804136276245, 0.060636136680841446, 0.08676747232675552, 0.04988805204629898, 0.08321600407361984, 0.04936002567410469, 0.0, 0.0, 0.0, 0.0], [0.056644245982170105, 0.06479756534099579, 0.045336753129959106, 0.14014272391796112, 0.044881995767354965, 0.024764863774180412, 0.09165864437818527, 0.02602609433233738, 0.04878808557987213, 0.05286514386534691, 0.11034878343343735, 0.05876830220222473, 0.062154434621334076, 0.03467698022723198, 0.05747754126787186, 0.04168203845620155, 0.03898577392101288, 0.0, 0.0, 0.0], [0.08902854472398758, 0.07715052366256714, 0.03735726699233055, 0.06924481689929962, 0.04119211062788963, 0.01692177727818489, 0.056606195867061615, 0.043395474553108215, 0.03798580542206764, 0.06101350486278534, 0.06327841430902481, 0.11598086357116699, 0.037598684430122375, 0.05029338598251343, 0.027607804164290428, 0.06265990436077118, 0.03746987134218216, 0.07521504908800125, 0.0, 0.0], [0.03647185489535332, 0.033483974635601044, 0.024332556873559952, 0.0703783854842186, 0.03472709655761719, 0.013935458846390247, 0.05279511585831642, 0.051772523671388626, 0.060333721339702606, 0.06646140664815903, 0.054271355271339417, 0.06483529508113861, 0.041216861456632614, 0.03451095521450043, 0.09309148788452148, 0.0464804545044899, 0.02026740461587906, 0.09857221692800522, 0.10206186026334763, 0.0], [0.023708919063210487, 0.024362333118915558, 0.06457309424877167, 0.1402725726366043, 0.02723672240972519, 0.034289222210645676, 0.0331537090241909, 0.01784561760723591, 0.04288215935230255, 0.016446396708488464, 0.09905210882425308, 0.03928907588124275, 0.0668564960360527, 0.03602944314479828, 0.12257242202758789, 0.04535125195980072, 0.027008097618818283, 0.07367316633462906, 0.03320317342877388, 0.03219408541917801]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6050756573677063, 0.3949243426322937, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.26647046208381653, 0.45329776406288147, 0.280231773853302, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.19991514086723328, 0.1808338165283203, 0.30381712317466736, 0.31543394923210144, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.14708472788333893, 0.12257640808820724, 0.2067817896604538, 0.33202195167541504, 0.19153505563735962, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1459883600473404, 0.1504419595003128, 0.2733045816421509, 0.10841173678636551, 0.18242403864860535, 0.13942933082580566, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.16316257417201996, 0.12501278519630432, 0.2389545887708664, 0.1872376948595047, 0.09102746844291687, 0.033127643167972565, 0.1614772379398346, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.11651010811328888, 0.10987013578414917, 0.16368155181407928, 0.23796264827251434, 0.06551270931959152, 0.09044311195611954, 0.13177335262298584, 0.08424627035856247, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.128031387925148, 0.14574068784713745, 0.20920762419700623, 0.08539390563964844, 0.036166273057460785, 0.09049634635448456, 0.100409597158432, 0.13294000923633575, 0.07161416113376617, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.07564037293195724, 0.05346810817718506, 0.0609370581805706, 0.10932061076164246, 0.11149124056100845, 0.023072944954037666, 0.12056983262300491, 0.254133015871048, 0.08193890005350113, 0.10942787677049637, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.06117846444249153, 0.038462910801172256, 0.06989502161741257, 0.07862240076065063, 0.08727364242076874, 0.03600899875164032, 0.09282409399747849, 0.18559619784355164, 0.09109711647033691, 0.1480662077665329, 0.11097495257854462, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.05088559910655022, 0.039968762546777725, 0.05393538996577263, 0.08471982926130295, 0.06591463088989258, 0.056574415415525436, 0.06971313804388046, 0.09639953076839447, 0.08636204153299332, 0.09390007704496384, 0.16457051038742065, 0.13705603778362274, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.03813827037811279, 0.03269079327583313, 0.09199532866477966, 0.04621094837784767, 0.09585575014352798, 0.018420815467834473, 0.1292731612920761, 0.06261499226093292, 0.036231353878974915, 0.047715120017528534, 0.23934665322303772, 0.08383528888225555, 0.07767147570848465, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0956002026796341, 0.1365322768688202, 0.09085879474878311, 0.08924777805805206, 0.05002880468964577, 0.04491390287876129, 0.10637696087360382, 0.045844584703445435, 0.06482924520969391, 0.04188684746623039, 0.03956189379096031, 0.03635502606630325, 0.036799971014261246, 0.12116368860006332, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.05095230042934418, 0.05879409611225128, 0.10363862663507462, 0.06232978031039238, 0.04712190851569176, 0.05057648569345474, 0.07236672937870026, 0.07104288041591644, 0.06053941696882248, 0.059321511536836624, 0.09738244861364365, 0.08675528317689896, 0.05139761418104172, 0.06743424385786057, 0.060346703976392746, 0.0, 0.0, 0.0, 0.0, 0.0], [0.09501954168081284, 0.0912516638636589, 0.08375809341669083, 0.07647785544395447, 0.06548868119716644, 0.03864042088389397, 0.07363075017929077, 0.058384284377098083, 0.0284165907651186, 0.048116862773895264, 0.06668896973133087, 0.09041129052639008, 0.03488975763320923, 0.04315778985619545, 0.041165973991155624, 0.06450141221284866, 0.0, 0.0, 0.0, 0.0], [0.040063001215457916, 0.03627951443195343, 0.02740260399878025, 0.05257778614759445, 0.0814889520406723, 0.015068373642861843, 0.053402379155159, 0.06868880242109299, 0.02955116517841816, 0.03578894957900047, 0.07437434047460556, 0.15500155091285706, 0.03680147975683212, 0.0263344869017601, 0.07267162203788757, 0.13898004591464996, 0.05552499368786812, 0.0, 0.0, 0.0], [0.033851806074380875, 0.03651456534862518, 0.04337097331881523, 0.049564436078071594, 0.0642884150147438, 0.016369448974728584, 0.06957786530256271, 0.09275904297828674, 0.04239350184798241, 0.06913357228040695, 0.07653506845235825, 0.10431855171918869, 0.03365272283554077, 0.03425270691514015, 0.05063536763191223, 0.07422482967376709, 0.040596429258584976, 0.06796067208051682, 0.0, 0.0], [0.024582814425230026, 0.014260043390095234, 0.018584605306386948, 0.03539887070655823, 0.02338082529604435, 0.014401775784790516, 0.02770639769732952, 0.06253589689731598, 0.044875357300043106, 0.06795374304056168, 0.03706807270646095, 0.16026952862739563, 0.029711369425058365, 0.012147796340286732, 0.2037382572889328, 0.025198692455887794, 0.019645091146230698, 0.11181148141622543, 0.06672947108745575, 0.0], [0.054769378155469894, 0.05249251425266266, 0.032816383987665176, 0.05176137387752533, 0.09805531054735184, 0.05175991728901863, 0.03728723153471947, 0.030828390270471573, 0.04694833606481552, 0.04589218273758888, 0.05053578317165375, 0.041567351669073105, 0.06452734023332596, 0.059667617082595825, 0.04377148300409317, 0.07285812497138977, 0.04811309278011322, 0.038517653942108154, 0.04305456951260567, 0.034775953739881516]]], \"tokens\": [\"1\", \"1\", \"2\", \"3\", \"5\", \"8\", \"13\", \"21\", \"34\", \"55\", \"89\", \"44\", \"33\", \"77\", \"10\", \"87\", \"97\", \"84\", \"81\", \"65\"]}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x7f009a54bb80>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(cache))\n",
    "attention_pattern = cache[\"pattern\", 0, \"attn\"]\n",
    "attention_pattern = attention_pattern.squeeze(0)\n",
    "print(attention_pattern.shape)\n",
    "\n",
    "token_labels = [str(int(t)) for t in example_input]\n",
    "\n",
    "print(\"Layer 0 Head Attention Patterns:\")\n",
    "cv.attention.attention_heads(tokens=token_labels, attention=attention_pattern)\n",
    "\n",
    "# Use the following if outputs not displaying in notebook or if you want to save the html file\n",
    "#html = cv.attention.attention_heads(tokens=list(map(str, example_input)), attention=attention_pattern)\n",
    "#with open(\"cv_attn_2.html\", \"w\") as f:\n",
    "#    f.write(str(html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('arena')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e28c680d33f95a364b6d7e112cefa96ea26c04ddac857c82a143b1aa5b3dfb2e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
